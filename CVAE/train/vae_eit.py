# -*- coding: utf-8 -*-"""Stage1: VAE:encoder+decoder. encoder for the second stage and decoder for the three stage"""import os# os.environ['CUDA_VISIBLE_DEVICES'] = '1'import numpy as npfrom dival.measure import PSNR, SSIMfrom matplotlib import pyplot as pltimport torch.optim as optimimport tqdmfrom vae import *from dataset_paper import xs_train, xs_valfrom torch.utils.tensorboard import SummaryWriterimport imlib as im# device# device = torch.device("cuda")device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')epochs=350Learning_rate = [0.0003,0.0007]batch_size = 8for learning_rate in Learning_rate:    x_train_loader = torch.utils.data.DataLoader(xs_train, batch_size, shuffle=True,drop_last=True)    x_val_loader = torch.utils.data.DataLoader(xs_val, batch_size, shuffle=False,drop_last=True)    vae_writer = SummaryWriter("path_{}_{}_{}".format(epochs, learning_rate, batch_size)) # change to your own path    vae = VAE(image_channels=1).to(device)    optimizer = optim.Adam(vae.parameters(), lr=learning_rate)    for epoch in tqdm.trange(epochs, desc='Epoch Loop'):        with tqdm.tqdm(x_train_loader, total=x_train_loader.batch_sampler.__len__()) as t:            vae.train(mode=True)            for idx, images in enumerate(x_train_loader):                images = images.to(device)                recon_images, mu, logvar = vae(images)                loss, mse, kld = loss_fn(recon_images, images, mu, logvar, batch_size)                optimizer.zero_grad()                loss.backward()                optimizer.step()                vae_writer.add_scalar("train_lr", optimizer.param_groups[0]["lr"], epoch)                t.set_description(f'Epoch {epoch}')                t.set_postfix(ordered_dict={'Loss': loss.item(),                                            'MSE': mse.item(),                                            'KLD': kld.item()})                t.update(1)            vae_writer.add_scalar("train_loss", loss, epoch)            # imm=im.immerge(recon_images.data.squeeze().cpu(),8,8)            # imm = imm.reshape((1,1024,1024))            # vae_writer.add_image("recon_images", imm, epoch)            # vae_writer.add_images("train_recon_eit", recon_images, epoch)            # vae_writer.add_embedding(mat=zz, global_step=epoch)            # for name, layer in vae.named_parameters():            #     vae_writer.add_histogram(name + '_grad', layer.grad.cpu().data.numpy(), epoch)            #     vae_writer.add_histogram(name + '_data', layer.cpu().data.numpy(), epoch)            vae.eval()            with torch.no_grad():                for idx, images in enumerate(x_val_loader):                    images = images.to(device)                    val_recon_images, val_mu, val_logvar = vae(images)                    val_loss, val_mse, val_kld = loss_fn(val_recon_images, images, val_mu, val_logvar, batch_size)                psnr_mean = np.mean([PSNR(xi.cpu(), gti.cpu()) for (xi, gti) in zip(val_recon_images, images)])                ssim_mean = np.mean(                    [SSIM(xi[0].cpu().numpy(), gti[0].cpu().numpy()) for (xi, gti) in zip(val_recon_images, images)])                mse_mean = np.mean(                    [((xi.cpu() - gti.cpu()) ** 2).mean() for (xi, gti) in zip(val_recon_images, images)])                # metrics = {'psnr': psnr_mean, 'ssim': ssim_mean, 'mse': mse_mean}                vae_writer.add_scalar("psnr_mean", psnr_mean, epoch)                vae_writer.add_scalar("ssim_mean", ssim_mean, epoch)                vae_writer.add_scalar("mse_mean", mse_mean, epoch)                vae_writer.add_scalar("val_mse", val_mse, epoch)                # train_im = im.immerge(recon_images.data.squeeze().cpu(),8,8)                # test_im = im.immerge(test_recon_images.data.squeeze().cpu(),8,8)                # fig0, ax =plt.subplots(1,2,figsize=(30,15))                # a0 = ax[0].imshow(train_im.reshape(1024,1024), vmin=0, vmax=1.5)                # a1 = ax[1].imshow(test_im.reshape(1024,1024), vmin=0, vmax=1.5)                # fig0.colorbar(a0, ax=ax, orientation='vertical', fraction=.1, pad=0.025,                #              ticks=[0, 0.3, 0.6, 0.9, 1.2, 1.5])                # vae_writer.add_figure("train&test", fig0, epoch)                imm = im.immerge(val_recon_images.data.squeeze().cpu(), 1, 8)                imm = imm.reshape((1,128,1024))                de_z = vae.sample(8)                de_zz = im.immerge(de_z.data.squeeze().cpu(), 1, 8)                de_zz = de_zz.reshape((1, 128, 1024))                # vae_writer.add_image("de_zz", de_zz, epoch)                # hz = torch.normal(size=[16, 1, 1, 16], mean=0, std=1.0, device=device)                fig, ax = plt.subplots(2, 1,                                       gridspec_kw={                                           'height_ratios': [1, 1]},figsize=(15,15))                a1 = ax[1].imshow(imm.reshape(128, 1024),vmin=0,vmax=1.5)                a0 = ax[0].imshow(de_zz.reshape(128,1024),vmin=0,vmax=1.5)                fig.colorbar(a0, ax=ax, orientation='horizontal', fraction=.1,pad=0.025,ticks=[0,0.3,0.6,0.9,1.2,1.5])                vae_writer.add_figure("re", fig, epoch)    torch.save(vae, "path_{}_{}_{}.pt".format(epochs, learning_rate, batch_size)) # change to your own path    vae_writer.close()